@def title = "Lecture 1: Understanding our computers"
# Lecture 1: All you need to know about performance

Scientific computing is a combination of scientific applications, mathematical modeling and high performance computing.
The first lecture focuses on understanding the performance of our computing devices and how to measure it.

## Required knowledge
* Knows how to open a linux shell
* Knows how to open a Julia REPL

## What limits the computing speed?
It is well known that the mathematicall modeling and algorithm is important to how fast an algorithm can run. But this is not the only important factor that related to performance.
You implement your algorithm and by type a program using your keyboard, while verifying the inputs through your monitor. The string you typed is stored in the main memory. A compiler reads this string and translates it into a format that computers can execute: the binary, in an optimized way.
The binary encodes instructions, which can be an arithematic/logical operation on registers, transfering data from the main memory to registers and vise versa, or branching.
When you start executing the binary, the operating system creates a new process for it and allocated required computing time, memory and I/O resources for it.
Even if you may have multiple processes running on the same CPU core, the operating system can schedule them properly by deviding the computing time into slices of length several milliseconds.
The computing device crunch the instructions in the memory by loading them to the instruction register first.
Registers are the fastest "storage" shiped with CPU and works in the same clock rate as CPU.
When running an data manipulation instruction, i.e. arithmetic, logical or shift instructions, registers are the only memory it can operate on.
The size of a general purposed register in a 64-bit machine is typically 64 bit while some wide registers for SIMD can be serveral times larger.
There are only tens of such registers in a CPU.
Before doing any arithematic operation, CPU must load the data from the main memory to a register through the system bus, after executing the data manipulation instruction, the result is copied back to the main memory.

The performance is related to the algorithm implementation, the optimization done by the compiler, the scheduling mechanism of your operating system and how fast a processor process instruction.
The compiler transforms your code into its equivalent form (meaning giving the same output as the original form for any input) with given static information in your code. The static information includes the type information and constants. Generally speaking, the more static information the compiler knows about your computation, the faster code it can generate.
The computing device also matters. A modern computer works in the time unit of a CPU clock cycle, which is typically 0.3ns, providing an lower bound for the computing time of a single instruction.
Reducing the number of instructions is important but is not the only goal since some instructions may cost much more time than others.
In each time unit, a CPU core can do at most one instruction in principle. However, the data transfer instructions for manipulating the main memory usually cost much more than a CPU clock cycle.
The direct reason is the memory stick works in a much lower speed than CPU.
A more fundamental reason is speed limitation of information propagation, i.e. the speed of light.

> Quiz: Given information can not be propagated in a speed faster than the light and the distance between the memory stick and the CPU is 10cm, what is the lower bound of the time loading a data from the memory to a register?

To overcome the limitation given by the constraint of physical locallity, engineers designed a complicated 3-level caching system that exists in the majority of our modern computers.
Caches belongs to the static random access memory (SRAM), which is much faster to access than the main memory or the dynamic random access memory (DRAM) we talk about in our daily life.
![](/assets/arch.png)
As is illustrated in the figure, by the order of decreasing speed and increasing size, caches include fastest L1 cache working in the same speed as CPU, slower L2 cache, and the slowest L3 cache but still faster than the DRAM.
When loading data from the main memory to a register, the CPU looks up the L1 cache first. If the CPU cannot find the data it is looking for in the L1 cache (or cache miss), it checks the L2 cache and L3 cache in order.
Once the desired data is found in the main memory, not only the required data, but also its physical neighbors are loaded to the caches.
The wisdom behind the caching system is data locality, i.e. whenever a data is used, the data physically close to it has much higher probability to be used than the rest. Locality is particularly true when a program perform elementwise operations to an array with contiguous storage.

When loading a 32/64-bit data from the memory to a register, the computer loads a chunk of data to the caches.
The wisdom behind this design is the locallity of data using, i.e. a data is much more likely to be used if it is physically close to the data currently in use.

Given a specific computing device, operating system and compiler, a high-performance implementation should take the compiling and hardware features into consideration.

## Hands on: Get CPU and memory information
### The computing power of a CPU
In the linux system, the CPU information can be printed by typing `lscpu`.
```
$ lscpu
Architecture:            x86_64
  CPU op-mode(s):        32-bit, 64-bit
  Address sizes:         39 bits physical, 48 bits virtual
  Byte Order:            Little Endian
CPU(s):                  8
  On-line CPU(s) list:   0-7
Vendor ID:               GenuineIntel
  Model name:            Intel(R) Core(TM) i7-10510U CPU @ 1.80GHz
    CPU family:          6
    Model:               142
    Thread(s) per core:  2
    Core(s) per socket:  4
    Socket(s):           1
    Stepping:            12
    CPU max MHz:         4900.0000
    CPU min MHz:         400.0000
    BogoMIPS:            4599.93
    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mc
                         a cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss 
                         ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art
                          arch_perfmon pebs bts rep_good nopl xtopology nonstop_
                         tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cp
                         l vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1
                          sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsav
                         e avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault
                          epb invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced 
                         tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase t
                         sc_adjust sgx bmi1 avx2 smep bmi2 erms invpcid mpx rdse
                         ed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1
                          xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_
                         window hwp_epp md_clear flush_l1d arch_capabilities
Virtualization features: 
  Virtualization:        VT-x
Caches (sum of all):     
  L1d:                   128 KiB (4 instances)
  L1i:                   128 KiB (4 instances)
  L2:                    1 MiB (4 instances)
  L3:                    8 MiB (1 instance)
NUMA:                    
  NUMA node(s):          1
  NUMA node0 CPU(s):     0-7
Vulnerabilities:         
  Itlb multihit:         KVM: Mitigation: VMX disabled
  L1tf:                  Not affected
  Mds:                   Not affected
  Meltdown:              Not affected
  Mmio stale data:       Mitigation; Clear CPU buffers; SMT vulnerable
  Retbleed:              Mitigation; Enhanced IBRS
  Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl
                          and seccomp
  Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer
                          sanitization
  Spectre v2:            Mitigation; Enhanced IBRS, IBPB conditional, RSB fillin
                         g, PBRSB-eIBRS SW sequence
  Srbds:                 Mitigation; Microcode
  Tsx async abort:       Not affected
```

From the output of `lscpu`, one can see the L1d and L1i are the L1 caches for data and instructions, both having a size of 128 KB.
The L2 and L3 caches are larger, they have size 1 MB and 8 MB respectively.
Depending on the different energy strategy and the payload, the CPU clock rate can vary between 0.4 GHz and 4.9 GHz.
The flags tell the information about the instruction sets, among which the `avx2` instruction set is for the single instruction multiple data (SIMD) parallelism.
It uses a 256 wide register to pack 4 double precision floating point numbers or 8 single precision floating point numbers, and perform multiply and add operation on them in a single CPU clock cycle.

A CPU may have multiple cores, and its theoretical upper bound of computing power can be measured by the number of floating point operations your computing device and perform in one second, namely, in floating point operations per second (FLOPS). For the above CPU, the computing power can be estimated with
```
Computing power of a CPU = 2.9 GHz (CPU clock speed, we use the maximum Turbo frequency)
			  * 2 (multiplication and add can happen at the same CPU clock)
			  * 2 (number of instructions per cycle)
		      * 4 (avx instruction set has a 256 with register, it can
                   crunch 4 vectorized double precision floating point
				   operations at one CPU cycle)
              * 4 (number of cores)
			= ? GFLOPS
```

An easy way to check the single/double precision FLOPS is using the matrix multiplication.
Try the following code in a Julia REPL
```julia
julia> using LinearAlgebra

julia> LinearAlgebra.versioninfo()
BLAS: libblastrampoline.so (f2c_capable)
  --> /home/leo/.julia/juliaup/julia-1.9.0-beta2+0.x64.linux.gnu/bin/../lib/julia/libopenblas64_.so (ILP64)
Threading:
  Threads.threadpoolsize() = 1
  Threads.maxthreadid() = 1
  LinearAlgebra.BLAS.get_num_threads() = 1
Relevant environment variables:
  OPENBLAS_NUM_THREADS = 1

julia> n = 1000
1000

julia> A, B, C = randn(Float64, n, n), randn(Float64, n, n), zeros(Float64, n, n);

julia> BLAS.set_num_threads(1)
julia> @benchmark mul!($C, $A, $B)
BenchmarkTools.Trial: 108 samples with 1 evaluation.
 Range (min … max):  44.629 ms … 55.949 ms  ┊ GC (min … max): 0.00% … 0.00%
 Time  (median):     46.178 ms              ┊ GC (median):    0.00%
 Time  (mean ± σ):   46.571 ms ±  1.634 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%

        ▁▁█▅▁ ▁
  ▃▃▆▃▆██████▇█▇▇▅▅▃▁▃▄▁▁▃▃▁▄▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▃ ▃
  44.6 ms         Histogram: frequency by time        54.1 ms <

 Memory estimate: 0 bytes, allocs estimate: 0.

julia> BLAS.set_num_threads(4)

julia> @benchmark mul!($C, $A, $B)
BenchmarkTools.Trial: 299 samples with 1 evaluation.
 Range (min … max):  15.252 ms … 26.505 ms  ┊ GC (min … max): 0.00% … 0.00%
 Time  (median):     16.303 ms              ┊ GC (median):    0.00%
 Time  (mean ± σ):   16.716 ms ±  1.831 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%
```

It can be shown that the computing power of a single thread is at least $1000^3 \times 2 / 0.044629 \approx 44.8 {\rm GFLOPS}$.
The computing time of a carefully implemented matrix multiplication function is dominated by the $1000^3$ multiplication operations and the same amount of addition operations.
Dividing $1000 \times 2$ floating point operations in total by the amount of time in the the unit of seconds is the peak FLOPS.
Here we have used the minimum computing time because the system does not always allocate time slices to the program we want to benchmark, using the minimum time gives us the best approximate to the time we want to measure.

For the parallel execution, although we have 4 CPU cores, the 4-thread computation does not give us a 4 time speed up.
This may have different causes, the cores may share caches, the operating system may not allocate all resources to this specific computing task.


In addition, since we are using intel CPU, MKL is slightly faster than the default OpenBLAS.
If we repeat the above numerical experiment, we will get the following timing information, which measures the peak performance of a CPU more acurately.

```
julia> using MKL

julia> LinearAlgebra.versioninfo()
BLAS: libblastrampoline.so (f2c_capable)
  --> /home/leo/.julia/artifacts/347e4bf25d69805922225ce6bf819ef0b8715426/lib/libmkl_rt.so (ILP64)
  --> /home/leo/.julia/artifacts/347e4bf25d69805922225ce6bf819ef0b8715426/lib/libmkl_rt.so (LP64)
Threading:
  Threads.threadpoolsize() = 1
  Threads.maxthreadid() = 1
  LinearAlgebra.BLAS.get_num_threads() = 4
Relevant environment variables:
  OPENBLAS_NUM_THREADS = 1

julia> BLAS.set_num_threads(1)

julia> @benchmark mul!($C, $A, $B)
BenchmarkTools.Trial: 113 samples with 1 evaluation.
 Range (min … max):  42.150 ms … 50.687 ms  ┊ GC (min … max): 0.00% … 0.00%
 Time  (median):     44.179 ms              ┊ GC (median):    0.00%
 Time  (mean ± σ):   44.434 ms ±  1.435 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%

         ▅▅▂▂▂▅▂▂▅ ▂▃ █                                        
  ▄▄▁▇▇▄▇████████████▅█▇▄█▄▁▄▇▁▁▅▁▁▁▁▄▄▁▁▄▁▄▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▄ ▄
  42.1 ms         Histogram: frequency by time        50.4 ms <

 Memory estimate: 0 bytes, allocs estimate: 0.

julia> BLAS.set_num_threads(4)

julia> @benchmark mul!($C, $A, $B)
BenchmarkTools.Trial: 281 samples with 1 evaluation.
 Range (min … max):  14.882 ms … 26.696 ms  ┊ GC (min … max): 0.00% … 0.00%
 Time  (median):     15.791 ms              ┊ GC (median):    0.00%
 Time  (mean ± σ):   17.775 ms ±  3.766 ms  ┊ GC (mean ± σ):  0.00% ± 0.00%

    ▃██▄                                                       
  ▄▇█████▄▃▃▂▁▁▁▃▃▂▂▂▁▂▁▃▂▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▂▂▃▄▆▆▅▃▃ ▃
  14.9 ms         Histogram: frequency by time        25.4 ms <

 Memory estimate: 0 bytes, allocs estimate: 0.
```

### Storage Hierachy
```bash
$ lsmem
RANGE                                  SIZE  STATE REMOVABLE  BLOCK
0x0000000000000000-0x000000007fffffff    2G online       yes   0-15
0x0000000088000000-0x000000008fffffff  128M online       yes     17
0x0000000100000000-0x0000000a6fffffff 37.8G online       yes 32-333

Memory block size:       128M
Total online memory:    39.9G
Total offline memory:      0B
```

## References
