---
layout: post
title: Linear Algebra Autodiff (complex valued)
---
# Linear Algebra Autodiff (complex valued)

## Einsum

### Definition of Einsum

`einsum` is defined as

$$
O_{\vec o} = \sum\limits_{(\vec a \cup \vec b \cup \vec c \ldots) \backslash \vec o }A_{\vec a}B_{\vec b}C_{\vec c} \ldots,
$$

where $\vec a = a_1, a_2\dots$ are labels that appear in tensor $A$, $\vec a\cup \vec b$ means the union of two sets of labels, $\vec a\backslash \vec b$ means setdiff between two sets of labels. The above sumation runs over all indices that does not appear in output tensor $O$.

## Autodiff

Given $\overline O$, In order to to obtain $\overline B \equiv \partial \mathcal{L}/\partial B$, consider the the diff rule

$$
\begin{align}
\delta \mathcal{L} &= \sum\limits_{\vec o} \overline O_{\vec o} \delta O_{\vec o} \\
&=\sum\limits_{\vec o\cup\vec a \cup \vec b\cup \vec c \ldots} \overline O_{\vec o}A_{\vec a}\delta B_{\vec b}C_{\vec c} \ldots
\end{align}
$$

Here, we have used the (partial) differential equation

$$
\delta O_{\vec o} = \sum\limits_{(\vec a \cup \vec b \cup \vec c \ldots) \backslash \vec o }A_{\vec a}\delta B_{\vec b}C_{\vec c} \ldots
$$

Then we define

$$
\overline B_{\vec b} = \sum\limits_{(\vec a \cup \vec b \cup \vec c \ldots) \backslash \vec b }A_{\vec a}\overline O_{\vec o}C_{\vec c} \ldots,
$$

We can readily verify

$$
\delta \mathcal{L} = \sum\limits_{\vec b} \overline B_{\vec b} \delta B_{\vec b}
$$

This backward rule is exactly an einsum that exchange output tensor $O$ and input tensor $B$.

In conclusion, the `index magic` of exchanging indices as backward rule holds for einsum.

Thank [Andreas Peter](https://github.com/under-Peter) for helpful discussion.

## Singular Value Decomposition (SVD)

*references*:

1. https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf 
2. https://j-towns.github.io/papers/svd-derivative.pdf

Complex valued SVD is defined as $A = USV^\dagger$. For simplicity, we consider a **full rank square matrix** $A$.
Differentiation gives

$$
dA = dUSV^\dagger + U dS V^\dagger + USdV^\dagger
$$

$$
U^\dagger dA V = U^\dagger dU S + dS + SdV^\dagger V
$$

Defining matrices $dC=U^\dagger dU$ and $dD = V^\dagger dV$ and $dP = U^\dagger dA V$, then we have

$$
\begin{cases}dC^\dagger+dC=0,\\dD^\dagger +dD=0\end{cases}
$$

We have

$$
dP = dC S + dS + SdD
$$

where $dCS$ and $SdD$ has zero diagonal elements. So that $dS = {\rm diag}(dP)$.

$$
\begin{align}
d\mathcal{L} &= {\rm Tr}\left[\overline{A}^TdA+\overline{A^*}^TdA^*\right]\\
&= {\rm Tr}\left[\overline{A}^TdA+dA^\dagger\overline{A}^*\right] ~~~~~~~\#rule~3
\end{align}
$$

Easy to show $\overline A_s = U^*\overline SV^T$.

Using the relations $dC^\dagger+dC=0$ and $dD^\dagger+dD=0$ 

$$
\begin{cases}
dPS + SdP^\dagger &= dC S^2-S^2dC\\
SdP + dP^\dagger S &= S^2dD-dD S^2
\end{cases}
$$

$$
\begin{cases}
dC = F\circ(dPS+SdP^\dagger)\\
dD = -F\circ (SdP+dP^\dagger S)
\end{cases}
$$

where $F_{ij} = \frac{1}{s_j^2-s_i^2}$, easy to verify $F^T = -F$.

First consider the contribution from $dU$

$$
d\mathcal L = {\rm Tr}\left[\overline{U}^TdU+h.c. \right]
$$

$$
\begin{align}
{\rm Tr}\overline U^TdU &= {\rm Tr} \overline U ^TU dC + \overline U^T (I-UU^\dagger) dAVS^{-1}\\
&= {\rm Tr}\overline U^T U (F\circ(dPS+SdP^\dagger))\\
 &=  {\rm Tr}(dPS+SdP^\dagger)(-F\circ (\overline U^T U))~~~~~~~~~~~~~~\# rule~1,2\\
 &= {\rm Tr}(dPS+SdP^\dagger)J^T
\end{align}
$$

Here, we defined $J=F\circ(U^T\overline U)$.

$$
\begin{align}
d\mathcal L &= {\rm Tr} (dPS+SdP^\dagger)(J+J^\dagger)^T\\
&= {\rm Tr} dPS(J+J^\dagger)^T+h.c.\\
&= {\rm Tr} U^\dagger dA V S(J+J^\dagger)^T+h.c.\\
&= {\rm Tr}\left[ VS(J+J^\dagger)^TU^\dagger\right] dA+h.c.
\end{align}
$$

By comparing with $d\mathcal L = {\rm Tr}\left[\overline{A}^TdA+h.c. \right]$, we have

$$
\begin{align}
\bar A_U &=  \left[VS(J+J^\dagger)^TU^\dagger\right]^T\\
&=U^*(J+J^\dagger)SV^T
\end{align}
$$

When $U$ is not full rank, this formula should take an extra term (Ref. 2)

$$
\begin{align}
\bar A_U &=U^*(J+J^\dagger)SV^T + (VS^{-1}dU^T(I-UU^\dagger))^T
\end{align}
$$


Similarly, for $V$ we have

$$
\begin{align}
\overline A_V &=U^*S(K+K^\dagger)V^T + U^* S^{-1} dV^T (I - VV^\dagger)),
\end{align}
$$

where $K=F\circ(V^TdV)$.

### Rules

rule 1. ${\rm Tr} \left[A(C\circ B\right)] = \sum A^T\circ C\circ B = {\rm Tr} ((C\circ A^T)^TB)={\rm Tr}(C^T\circ A)B$

rule2. $(C\circ A)^T = C^T \circ A^T$

rule3. When $\mathcal L$ is real, $\frac{\partial \mathcal{L}}{\partial x^*} =  \left(\frac{\partial \mathcal{L}}{\partial x}\right)^*$

### How to Test SVD

e.g. To test the adjoint contribution from $U$, we can construct a gauge insensitive  test function

```julia
# H is a random Hermitian Matrix
function loss(A)
    U, S, V = svd(A)
    psi = U[:,1]
    psi'*H*psi
end

function gradient(A)
    U, S, V = svd(A)
    dU = zero(U)
    dS = zero(S)
    dV = zero(V)
    dU[:,1] = U[:,1]'*H
    dA = svd_back(U, S, V, dU, dS, dV)
    dA
end
```

