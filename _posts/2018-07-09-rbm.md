---
layout: post
title: Restricted Boltzmann Machine (RBM) for Physicsts
---
# Restricted Boltzmann Machine (RBM) for Physicsts

Give a pdf $\pi(x)$, $x$ is an input vector (or image), it can be

* distribution of an image dataset in its Hilbert space.
* monte carlo sampling distribution
* measurement results of a quantum circuit

## What is Boltzmann Machine?

Define an energy based model

$p(x)=\frac{e^{-E(x)}}{Z}$, where $E(x)$ is described by a graph, $Z=\sum\limits_xe^{-E(x)}$ and $x=0,1$.

$E(x) = -\frac 1 2x_iW_{ij}x_j$

[picture of boltzmann machine, as an example]![bmrbm](Figures/Handson/bmrbm.png)

This is Boltzmann machine!

## What is the motivation of Boltzmann Machine?

we want $p(x)\sim\pi(x)$, Given i.i.d. data $y\sim\pi(x)$.

Quantify our target to Loss!

$L = -\prod\limits_yp(y)$, we want to maximize this, equivalently minimize 

$$\begin{align}\mathcal{L}=\frac L N = \frac 1 N \sum\limits_y E(y) +\log Z\end{align}$$

This is the famous NLL loss!

## How to minimize the loss?

Partition function $Z$ is notoriously hard to obtain, it is one of the essential problem in statistic physics.

Interestingly, $\frac{\partial \mathcal L}{\partial \theta}$ can be estimated!

$$\begin{align}\frac{\partial L}{\partial\theta} &= \mathbb E_{y\in \mathcal D}[E'(y)]-\frac{\sum\limits_x E'(x)e^{-E(x)}}{Z}\\&=\mathbb E_{y\in \mathcal D}[E'(y)] - \mathbb E_{x\sim p(x)}[E'(x)]\end{align}$$

However, classical spin statistics are not simple enough, the second term is still hard.

### A simplified model RBM suited for inference

$$E(x) = x^T\mathbf Wh + b^Th +x^Ta$$

![bmrbm](Figures/Handson/rbmcompress.png)





conditional sampling $p(x\vert h)\propto e^{-x^T(\mathbf Wh+a)}=\prod\limits_ie^{-x_i\Theta_i}$, where $\Theta_i$ is the ith element of $\mathbf Wh+a$, so that we can do pixel-wise sampling according to probability $p(x_i)\propto \frac{e^{-x_i\Theta_i}}{1+e^{-\Theta_i}}$  (i.e. $p(x_i=0)=\sigma(\Theta_i)$)

**How can this be useful?**

Inference!

given part of $x$, e.g. half of an image, part of dataset, guess rest data.

$p(x_{B}\vert x_{A}) = \sum\limits_hp(x_{B}\vert h)p(h\vert x_{A})$

useful in recommender systems.

**Gibbs sampling:**

conditional sampling $x_1\rightarrow h_1\rightarrow x_2 \rightarrow \ldots\rightarrow x_n$, will converge to $p(x)$ and $p(h)$.

1). $\frac{p(x_{t}\vert x_{t-1})}{p(x_{t-1}\vert x_t)}=\frac{p(x_t)}{p(x_{t-1})}$

$p(x_t\vert x_{t-1}) =\sum\limits_{h} p(x_t\vert h)p(h\vert x_{t-1})=\sum\limits_h\frac{p(x_t, h)p(h, x_{t-1})}{p(h)p(x_{t-1})}$

$p(x_{t-1}\vert x_{t}) =\sum\limits_{h} p(x_{t-1}\vert h)p(h\vert x_{t})=\sum\limits_h\frac{p(x_{t-1}, h)p(h, x_t)}{p(h)p(x_t)}$

Statistic ensemble $\rightarrow$ Time ensemble

2). ergodicity, obvious.
