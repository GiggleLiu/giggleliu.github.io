<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
   <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
   <link rel="stylesheet" href="/libs/highlight/styles/github.min.css">
   
  <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/poole_hyde.css">
<!-- style adjustments -->
<style>
  html {font-size: 17px;}
  .franklin-content {position: relative; padding-left: 8%; padding-right: 5%; line-height: 1.35em;}
  @media (min-width: 940px) {
    .franklin-content {width: 100%; margin-left: auto; margin-right: auto;}
  }
  @media (max-width: 768px) {
    .franklin-content {padding-left: 6%; padding-right: 6%;}
  }
</style>
<link rel="icon" href="/assets/favicon.png">

   <title>Linear Algebra Autodiff (complex valued</title>  
</head>
<body>
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1><a href="/">Jinguo LIU</a></h1>
      <p class="lead">Early to bed and early to rise, makes a man healthy, wealthy, and wise.</p>
    </div>
    <nav class="sidebar-nav">
      <a class="sidebar-nav-item " href="/">Home</a>
      <a class="sidebar-nav-item " href="/blogs/">Blogs</a>
      <a class="sidebar-nav-item " href="/code/">Code</a>
      <a class="sidebar-nav-item " href="/about/">About Me</a>
    </nav>
    <p>&copy; GiggleLiu.</p>
  </div>
</div>
<div class="content container">

<!-- Content appended here -->
<div class="franklin-content"><h1 id="linear_algebra_autodiff_complex_valued"><a href="#linear_algebra_autodiff_complex_valued" class="header-anchor">Linear Algebra Autodiff &#40;complex valued&#41;</a></h1>
<p>You can find the Julia implementations in <a href="https://github.com/GiggleLiu/BackwardsLinalg.jl">BackwardsLinalg.jl</a> and <a href="https://github.com/under-Peter/OMEinsum.jl">OMEinsum.jl</a>.</p>
<h2 id="einsum"><a href="#einsum" class="header-anchor">Einsum</a></h2>
<h3 id="definition_of_einsum"><a href="#definition_of_einsum" class="header-anchor">Definition of Einsum</a></h3>
<p><code>einsum</code> is defined as</p>
\[
O_{\vec o} = \sum\limits_{(\vec a \cup \vec b \cup \vec c \ldots) \backslash \vec o }A_{\vec a}B_{\vec b}C_{\vec c} \ldots,
\]
<p>where \(\vec a = a_1, a_2\dots\) are labels that appear in tensor \(A\), \(\vec a\cup \vec b\) means the union of two sets of labels, \(\vec a\backslash \vec b\) means setdiff between two sets of labels. The above sumation runs over all indices that does not appear in output tensor \(O\).</p>
<h2 id="autodiff"><a href="#autodiff" class="header-anchor">Autodiff</a></h2>
<p>Given \(\overline O\), In order to to obtain \(\overline B \equiv \partial \mathcal{L}/\partial B\), consider the the diff rule</p>
\[
\begin{align}
\delta \mathcal{L} &= \sum\limits_{\vec o} \overline O_{\vec o} \delta O_{\vec o} \\
&=\sum\limits_{\vec o\cup\vec a \cup \vec b\cup \vec c \ldots} \overline O_{\vec o}A_{\vec a}\delta B_{\vec b}C_{\vec c} \ldots
\end{align}
\]
<p>Here, we have used the &#40;partial&#41; differential equation</p>
\[
\delta O_{\vec o} = \sum\limits_{(\vec a \cup \vec b \cup \vec c \ldots) \backslash \vec o }A_{\vec a}\delta B_{\vec b}C_{\vec c} \ldots
\]
<p>Then we define</p>
\[
\overline B_{\vec b} = \sum\limits_{(\vec a \cup \vec b \cup \vec c \ldots) \backslash \vec b }A_{\vec a}\overline O_{\vec o}C_{\vec c} \ldots,
\]
<p>We can readily verify</p>
\[
\delta \mathcal{L} = \sum\limits_{\vec b} \overline B_{\vec b} \delta B_{\vec b}
\]
<p>This backward rule is exactly an einsum that exchange output tensor \(O\) and input tensor \(B\).</p>
<p>In conclusion, the <code>index magic</code> of exchanging indices as backward rule holds for einsum.</p>
<p>Thank <a href="https://github.com/under-Peter">Andreas Peter</a> for helpful discussion.</p>
<h2 id="symmetric_eigenvalue_decomposition_ed"><a href="#symmetric_eigenvalue_decomposition_ed" class="header-anchor">Symmetric Eigenvalue Decomposition &#40;ED&#41;</a></h2>
<p><em>references</em>:</p>
<ol>
<li><p><a href="https://arxiv.org/abs/1710.08717">https://arxiv.org/abs/1710.08717</a></p>
</li>
</ol>
\[
A = UEU^\dagger
\]
<p>We have</p>
\[
\overline{A} = U\left[\overline{E} + \frac{1}{2}\left(\overline{U}^\dagger U \circ F + h.c.\right)\right]U^\dagger
\]
<p>Where \(F_{ij}=(E_j- E_i)^{-1}\).</p>
<p>If \(E\) is continuous, we define the density \(\rho(E) = \sum\limits_k \delta(E-E_k)=-\frac{1}{\pi}\int_k \Im[G^r(E, k)] \) &#40;check sign&#33;&#41;. Where \(G^r(E, k) = \frac{1}{E-E_k+i\delta}\).</p>
<p>We have</p>
\[
\overline{A} = U\left[\overline{E} + \frac{1}{2}\left(\overline{U}^\dagger U \circ \Re [G(E_i, E_j)] + h.c.\right)\right]U^\dagger
\]
<h2 id="singular_value_decomposition_svd"><a href="#singular_value_decomposition_svd" class="header-anchor">Singular Value Decomposition &#40;SVD&#41;</a></h2>
<p><em>references</em>:</p>
<ol>
<li><p><a href="https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf">https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf</a></p>
</li>
<li><p><a href="https://j-towns.github.io/papers/svd-derivative.pdf">https://j-towns.github.io/papers/svd-derivative.pdf</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1907.13422">https://arxiv.org/abs/1907.13422</a></p>
</li>
</ol>
<p>Complex valued SVD is defined as \(A = USV^\dagger\). For simplicity, we consider a <strong>full rank square matrix</strong> \(A\). Differentiation gives</p>
\[
dA = dUSV^\dagger + U dS V^\dagger + USdV^\dagger
\]
\[
U^\dagger dA V = U^\dagger dU S + dS + SdV^\dagger V
\]
<p>Defining matrices \(dC=U^\dagger dU\) and \(dD = dV^\dagger V\) and \(dP = U^\dagger dA V\), then we have</p>
\[
\begin{cases}dC^\dagger+dC=0,\\dD^\dagger +dD=0\end{cases}
\]
<p>We have</p>
\[
dP = dC S + dS + SdD
\]
<p>where \(dCS\) and \(SdD\) has zero real part in diagonal elements. So that \(dS = \Re[{\rm diag}(dP)]\). </p>
\[
\begin{align}
d\mathcal{L} &= {\rm Tr}\left[\overline{A}^TdA+\overline{A^*}^TdA^*\right]\\
&= {\rm Tr}\left[\overline{A}^TdA+dA^\dagger\overline{A}^*\right] ~~~~~~~\#rule~3
\end{align}
\]
<p>~#rule~3 \end&#123;align&#125; &#36;</p>
<p>Easy to show \(\overline A_s = U^*\overline SV^T\). Notice here, \(\overline A\) is the <strong>derivative</strong> rather than <strong>gradient</strong>, they are different by a conjugate, this is why we have transpose rather than conjugate here. see my <a href="https://giggleliu.github.io/2018/02/01/complex_bp.html">complex valued autodiff blog</a> for detail.</p>
<p>Using the relations \(dC^\dagger+dC=0\) and \(dD^\dagger+dD=0\) </p>
\[
\begin{cases}
dPS + SdP^\dagger &= dC S^2-S^2dC\\
SdP + dP^\dagger S &= S^2dD-dD S^2
\end{cases}
\]
\[
\begin{cases}
dC = F\circ(dPS+SdP^\dagger)\\
dD = -F\circ (SdP+dP^\dagger S)
\end{cases}
\]
<p>where \(F_{ij} = \frac{1}{s_j^2-s_i^2}\), easy to verify \(F^T = -F\). Notice here, the relation between the imaginary diagonal parts  is lost</p>
\[
\color{red}{\Im[I\circ dP] = \Im[I\circ(dC+dD)]}
\]
<p>This <strong>the missing diagonal imaginary part</strong> is definitely not trivial, but has been ignored for a long time until <a href="https://github.com/tensorflow/tensorflow/issues/13641#issuecomment-526976200">@refraction-ray</a> &#40;Shixin Zhang&#41; mentioned and solved it. Let&#39;s first focus on the off-diagonal contributions from \(dU\)</p>
\[
\begin{align}
{\rm Tr}\overline U^TdU &= {\rm Tr} \overline U ^TU dC + \overline U^T (I-UU^\dagger) dAVS^{-1}\\
&= {\rm Tr}\overline U^T U (F\circ(dPS+SdP^\dagger))\\
 &=  {\rm Tr}(dPS+SdP^\dagger)(-F\circ (\overline U^T U)) \# rule~1,2\\
 &= {\rm Tr}(dPS+SdP^\dagger)J^T
\end{align}
\]
<p>Here, we defined \(J=F\circ(U^T\overline U)\).</p>
\[
\begin{align}
d\mathcal L &= {\rm Tr} (dPS+SdP^\dagger)(J+J^\dagger)^T\\
&= {\rm Tr} dPS(J+J^\dagger)^T+h.c.\\
&= {\rm Tr} U^\dagger dA V S(J+J^\dagger)^T+h.c.\\
&= {\rm Tr}\left[ VS(J+J^\dagger)^TU^\dagger\right] dA+h.c.
\end{align}
\]
<p>By comparing with \(d\mathcal L = {\rm Tr}\left[\overline{A}^TdA+h.c. \right]\), we have</p>
\[
\begin{align}
\bar A_U^{(\rm real)} &=  \left[VS(J+J^\dagger)^TU^\dagger\right]^T\\
&=U^*(J+J^\dagger)SV^T
\end{align}
\]
<h4 id="update_the_missing_diagonal_imaginary_part"><a href="#update_the_missing_diagonal_imaginary_part" class="header-anchor">Update: The missing diagonal imaginary part</a></h4>
<p>Now let&#39;s inspect the diagonal imaginary parts of \(dC\) and \(dD\) in Eq. 16. At a first glance, it is not sufficient to derive \(dC\) and \(dD\) from \(dP\), but consider there is still an information not used, <strong>the loss must be gauge invariant</strong>, which means</p>
\[
\mathcal{L}(U\Lambda, S, V\Lambda)
\]
<p>Should be independent of the choice of gauge \(\Lambda\), which is defined as \({\rm diag}(e^i\phi, ...)\)</p>
\[
\begin{align}
d\mathcal{L} &={\rm Tr}[ \overline{U\Lambda}^T d(U\Lambda) +\overline  SdS+\overline{V\Lambda}^Td(V\Lambda)] + h.c.\\
&={\rm Tr}[ \overline {U\Lambda}^T (dU\Lambda+Ud\Lambda) +\overline{S}dS+  \overline{V\Lambda}^T(Vd\Lambda +dV\Lambda)] + h.c.\\
&= {\rm Tr}[(\overline{U\Lambda}^TU+\overline{V\Lambda}^TV )d\Lambda ] + \ldots + h.c.
\end{align}
\]
<p>Gauge invariance refers to</p>
\[
\overline{\Lambda} =  I\circ(\overline{U\Lambda}^TU+\overline{V\Lambda}^TV) = 0
\]
<p>For any \(\Lambda\), where \(I\) refers to the diagonal mask matrix. It is of cause valid when \(\Lambda\rightarrow1\), \(I\circ(\overline{U}^TU+\overline V^TV) = 0\).</p>
<p>Consider the contribution from the <strong>diagonal imaginary part</strong>, we have</p>
\[
\begin{align}
&{\rm Tr} [\overline U^T U (I \circ \Im [dC])+\overline V^T V (I \circ \Im [dD^\dagger])] + h.c.\\
&={\rm Tr} [ I \circ (\overline U^T U)\Im [dC]-I\circ (\overline V^T V) \Im [dD]] +h.c. ~~~~~~~~~~~~~~\#  rule 1\\
&={\rm Tr} [ I \circ (\overline U^T U)(\Im [dC]+ \Im [dD])] \\
&={\rm Tr}[I\circ (\overline U^T U) \Im[dP]S^{-1}]  \\
&={\rm Tr}[S^{-1}\Lambda_J U^{\dagger}dA V]\\
\end{align}
\]
<p>~~#  rule 1\
&amp;&#61;&#123;\rm Tr&#125; &#91; I \circ &#40;\overline U^T U&#41;&#40;\Im &#91;dC&#93;&#43; \Im &#91;dD&#93;&#41;&#93; \
&amp;&#61;&#123;\rm Tr&#125;&#91;I\circ &#40;\overline U^T U&#41; \Im&#91;dP&#93;S^&#123;-1&#125;&#93;  \
&amp;&#61;&#123;\rm Tr&#125;&#91;S^&#123;-1&#125;\Lambda_J U^&#123;\dagger&#125;dA V&#93;\
\end&#123;align&#125; &#36;</p>
<p>where \(\Lambda_J  = \Im[I\circ(\overline U^TU)]= \frac 1 2I\circ(\overline U^TU)-h.c.\), with \(I\) the mask for diagonal part. Since only the real part contribute to \(\delta \mathcal{L}\) &#40;the imaginary part will be canceled by the Hermitian conjugate counterpart&#41;, we can safely move \(\Im\) from right to left.</p>
\[
\begin{align}
\color{red}{\bar A_{U+V}^{(\rm imag)} = U^*\Lambda_J S^{-1}V^T}
\end{align}
\]
<p><strong>Thanks  <a href="https://github.com/refraction-ray">@refraction-ray</a> &#40;Shixin Zhang&#41; for sharing his idea in the first time. This is the <a href="https://github.com/tensorflow/tensorflow/issues/13641#issuecomment-526976200">issue for discussion</a>. His arXiv preprint is coming out soon.</strong></p>
<p>When \(U\) is <strong>not full rank</strong>, this formula should take an extra term &#40;Ref. 2&#41;</p>
\[
\begin{align}
\bar A_U^{(\rm real)} &=U^*(J+J^\dagger)SV^T + (VS^{-1}\overline U^T(I-UU^\dagger))^T
\end{align}
\]
<p>Similarly, for \(V​\) we have</p>
\[
\begin{align}
\overline A_V^{(\rm real)} &=U^*S(K+K^\dagger)V^T + (U S^{-1} \overline V^T (I - VV^\dagger))^*,
\end{align}
\]
<p>where \(K=F\circ(V^T\overline V)​\).</p>
<p>To wrap up</p>
\[
\overline A = \overline A_U^{\rm (real)} + \overline A_S + \overline A_V^{\rm (real)} +  \overline A_{U+V}^{\rm (imag)}
\]
<p>This result can be directly used in <strong>autograd</strong>.</p>
<p>For the <strong>gradient</strong> used in training, one should change the convention</p>
\[
\mathcal{\overline A} = \overline A^*,\\ \mathcal{\overline U} = \overline U^*,\\ \mathcal{\overline V}= \overline V^*.
\]
<p>This convention is used in <strong>tensorflow</strong>, <strong>Zygote.jl</strong>. Which is</p>
\[
\begin{align}
\mathcal{\overline A} =& U(\mathcal{J}+\mathcal{J}^\dagger)SV^\dagger + (I-UU^\dagger)\mathcal{\overline U}S^{-1}V^\dagger\\
&+ U\overline SV^\dagger\\
&+US(\mathcal{K}+\mathcal{K}^\dagger)V^\dagger + U S^{-1} \mathcal{\overline V}^\dagger (I - VV^\dagger)\\
&\color{red}{+\frac 1 2 U (I\circ(U^\dagger\overline U)-h.c.)S^{-1}V^\dagger}
\end{align}
\]
<p>where \(J=F\circ(U^\dagger\mathcal{\overline U})\) and \(K=F\circ(V^\dagger \mathcal{\overline V})\).</p>
<h3 id="rules"><a href="#rules" class="header-anchor">Rules</a></h3>
<p>rule 1. \({\rm Tr} \left[A(C\circ B\right)] = \sum A^T\circ C\circ B = {\rm Tr} ((C\circ A^T)^TB)={\rm Tr}(C^T\circ A)B\)</p>
<p>rule2. \((C\circ A)^T = C^T \circ A^T\)</p>
<p>rule3. When \(\mathcal L\) is real, </p>
\[\frac{\partial \mathcal{L}}{\partial x^*} =  \left(\frac{\partial \mathcal{L}}{\partial x}\right)^*\]
<h3 id="how_to_test_svd"><a href="#how_to_test_svd" class="header-anchor">How to Test SVD</a></h3>
<p>e.g. To test the adjoint contribution from \(U\), we can construct a gauge insensitive  test function</p>
<pre><code class="language-julia"># H is a random Hermitian Matrix
function loss&#40;A&#41;
    U, S, V &#61; svd&#40;A&#41;
    psi &#61; U&#91;:,1&#93;
    psi&#39;*H*psi
end

function gradient&#40;A&#41;
    U, S, V &#61; svd&#40;A&#41;
    dU &#61; zero&#40;U&#41;
    dS &#61; zero&#40;S&#41;
    dV &#61; zero&#40;V&#41;
    dU&#91;:,1&#93; &#61; U&#91;:,1&#93;&#39;*H
    dA &#61; svd_back&#40;U, S, V, dU, dS, dV&#41;
    dA
end</code></pre>
<h2 id="qr_decomposition"><a href="#qr_decomposition" class="header-anchor">QR decomposition</a></h2>
<p><em>references</em>:</p>
<ol>
<li><p><a href="https://arxiv.org/abs/1710.08717">https://arxiv.org/abs/1710.08717</a></p>
</li>
<li><p><a href="https://arxiv.org/abs/1903.09650">https://arxiv.org/abs/1903.09650</a></p>
</li>
</ol>
\[
A = QR
\]
<p>with \(Q^\dagger Q = \mathbb{I}\), so that \(dQ^\dagger Q+Q^\dagger dQ=0\). \(R\) is a complex upper triangular matrix, with diagonal part real.</p>
\[
dA = dQR+QdR
\]
\[
dQ = dAR^{-1}-QdRR^{-1}
\]
\[
\begin{cases}
Q^\dagger dQ = dC - dRR^{-1}\\
dQ^\dagger Q =dC^\dagger - R^{-\dagger}dR^\dagger
\end{cases}
\]
<p>where \(dC=Q^\dagger dAR^{-1}\).</p>
<p>Then</p>
\[
dC+dC^\dagger = dRR^{-1} +(dRR^{-1})^\dagger
\]
<p>Notice \(dR\) is upper triangular and its diag is lower triangular, this restriction gives</p>
\[
U\circ(dC+dC^\dagger) = dRR^{-1}
\]
<p>where \(U\) is a mask operator that its element value is \(1\) for upper triangular part, \(0.5\) for diagonal part and \(0\) for lower triangular part. One should also notice here both \(R\) and \(dR\) has real diagonal parts, as well as the product \(dRR^{-1}\).</p>
<p>Now let&#39;s wrap up using the Zygote convension of gradient</p>
\[
\begin{align}
d\mathcal L &= {\rm Tr}\left[\overline{\mathcal{Q}}^\dagger dQ+\overline{\mathcal{R}}^\dagger dR +h.c. \right]\\
&={\rm Tr}\left[\overline{\mathcal{Q}}^\dagger dA R^{-1}-\overline{\mathcal{Q}}^\dagger QdR
R^{-1}+\overline{\mathcal{R}}^\dagger dR +h.c. \right]\\
&={\rm Tr}\left[ R^{-1}\overline{\mathcal{Q}}^\dagger dA+ R^{-1}(-\overline{\mathcal{Q}}^\dagger Q +R\overline{\mathcal{R}}^\dagger) dR +h.c. \right]\\
&={\rm Tr}\left[ R^{-1}\overline{\mathcal{Q}}^\dagger dA+ R^{-1}M dR +h.c. \right]
\end{align}
\]
<p>here, \(M=R\overline{\mathcal{R}}^\dagger-\overline{\mathcal{Q}}^\dagger Q\). Plug in \(dR\) we have</p>
\[
\begin{align}
d\mathcal{L}&={\rm Tr}\left[ R^{-1}\overline{\mathcal{Q}}^\dagger dA + M \left[U\circ(dC+dC^\dagger)\right] +h.c. \right]\\
&={\rm Tr}\left[ R^{-1}\overline{\mathcal{Q}}^\dagger dA + (M\circ L)(dC+dC^\dagger) +h.c. \right]  \;\;\# rule\; 1\\
&={\rm Tr}\left[ (R^{-1}\overline{\mathcal{Q}}^\dagger dA+h.c.) + (M\circ L)(dC + dC^\dagger)+ (M\circ L)^\dagger (dC + dC^\dagger)\right]\\
&={\rm Tr}\left[ R^{-1}\overline{\mathcal{Q}}^\dagger dA + (M\circ L+h.c.)dC + h.c.\right]\\
&={\rm Tr}\left[ R^{-1}\overline{\mathcal{Q}}^\dagger dA + (M\circ L+h.c.)Q^\dagger dAR^{-1}\right]+h.c.\\
\end{align}
\]
<p>where \(L =U^\dagger = 1-U\) is the mask of lower triangular part of a matrix.</p>
\[
\begin{align}
\mathcal{\overline A}^\dagger &= R^{-1}\left[\overline{\mathcal{Q}}^\dagger + (M\circ L+h.c.)Q^\dagger\right]\\
\mathcal{\overline A} &= \left[\overline{\mathcal{Q}} + Q(M\circ L+h.c.)\right]R^{-\dagger}\\
&=\left[\overline{\mathcal{Q}} + Q \texttt{copyltu}(M)\right]R^{-\dagger}
\end{align}
\]
<p>Here, the \(\texttt{copyltu}​\) takes conjugate when copying elements to upper triangular part.</p>
<div class="page-foot">
    <div>
    <a href="https://github.com/GiggleLiu" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github: GiggleLiu</a>
    <br>
    <a href="https://twitter.com/GiggleLiu" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i> Twitter: GiggleLiu</a>
    <br>
    <a href="https://scholar.google.com/citations?user=4edw228AAAAJ" rel="nofollow noopener noreferrer"><i class="fa-google-scholar" aria-hidden="true"></i> Google Scholar: 4edw228AAAAJ</a>
    </div>
    <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> GiggleLiu. Last modified: November 20, 2022.
    Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
</div>
</div><!-- CONTENT ENDS HERE -->
    </div>  <!-- div: content container -->
    <script defer src="https://use.fontawesome.com/releases/v5.8.2/js/all.js" integrity="sha384-DJ25uNYET2XCl5ZF++U8eNxPWqcKohUUBUpKGlNLMchM7q4Wjg2CUpjHLaL8yYPH" crossorigin="anonymous"></script>
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/contrib/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
        <script src="/libs/highlight/highlight.min.js"></script>
<script>hljs.highlightAll();hljs.configure({tabReplace: '    '});</script>

    

  </body>

</html>
